
<p><i>Entropy</i> is a measure of the energy of a system that is unavailable for doing useful work. In statistical thermodynamics, entropy (usual symbol <span class="texhtml mvar" style="font-style:italic;">S</span>) is a measure of the number of microscopic configurations <span class="texhtml">&#x3A9;</span> that correspond to a thermodynamic system in a state specified by certain macroscopic variables. Specifically, assuming that each of the microscopic configurations is equally probable, the entropy of the system is the natural logarithm of that number of configurations, multiplied by the Boltzmann constant <span class="texhtml"><i>k</i><sub>B</sub></span> (which provides consistency with the original thermodynamic concept of entropy discussed below, and gives entropy the dimension of energy divided by temperature). Formally,</p>